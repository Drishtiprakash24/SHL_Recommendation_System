{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb3d53-1ec8-4db7-8734-57f30a520e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# Load train/test\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Load vector DB created in 01\n",
    "vector_db = Chroma(persist_directory=\"shl_vector_db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c91768a-9522-4e57-8b1b-eaf53d767614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "shl_df = pd.read_csv(\"shl_full_database.csv\")\n",
    "\n",
    "shl_urls = shl_df[\"URL\"].tolist()\n",
    "shl_names = shl_df[\"Assessment Name\"].tolist()\n",
    "\n",
    "def match_name(url, url_list, name_list):\n",
    "    match = get_close_matches(url, url_list, n=1, cutoff=0.6)\n",
    "    if match:\n",
    "        return name_list[url_list.index(match[0])]\n",
    "    return None\n",
    "\n",
    "train_df[\"Assessment_name\"] = train_df[\"Assessment_url\"].apply(lambda x: match_name(x, shl_urls, shl_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2931406-d631-4738-bac3-b05cc49b0e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_recommendations(results, final_k=10):\n",
    "    type_dict = {\"K\": [], \"P\": [], \"A\": []}\n",
    "    for r in results:\n",
    "        ttype = r.metadata.get(\"test_type\", \"K\")\n",
    "        if ttype in type_dict:\n",
    "            type_dict[ttype].append(r)\n",
    "\n",
    "    num_types = len([v for v in type_dict.values() if v])\n",
    "    per_type = max(final_k // num_types, 1)\n",
    "\n",
    "    balanced = []\n",
    "    for ttype, items in type_dict.items():\n",
    "        balanced.extend(items[:per_type])\n",
    "\n",
    "    if len(balanced) < final_k:\n",
    "        remaining = [r for r in results if r not in balanced]\n",
    "        balanced.extend(remaining[:final_k - len(balanced)])\n",
    "\n",
    "    return balanced[:final_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fa64b-7c84-46b1-a9c3-85598b148da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Drishti Prakash\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|██████████████████████████████████████████████████████████████████████████████████████| 79.3M/79.3M [00:56<00:00, 1.47MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Recall@10 (Balanced): 0.5\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "recall_scores = []\n",
    "\n",
    "for query, group in train_df.groupby(\"Query\"):\n",
    "    true_names = group[\"Assessment_name\"].tolist()\n",
    "    results = vector_db.similarity_search(query, k=20)\n",
    "    balanced = balanced_recommendations(results, final_k=K)\n",
    "    retrieved_names = [r.metadata.get(\"name\", \"\").strip() for r in balanced]\n",
    "    \n",
    "    score = int(any(name in retrieved_names for name in true_names))\n",
    "    recall_scores.append(score)\n",
    "\n",
    "mean_recall_at_10 = sum(recall_scores) / len(recall_scores)\n",
    "print(\"Mean Recall@10 (Balanced):\", mean_recall_at_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a553f-3f2d-4806-a637-b0fad0b87b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission CSV created!\n"
     ]
    }
   ],
   "source": [
    "submission = []\n",
    "\n",
    "for query in test_df[\"Query\"]:\n",
    "    results = vector_db.similarity_search(query, k=20)\n",
    "    balanced = balanced_recommendations(results, final_k=10)\n",
    "    \n",
    "    for r in balanced:\n",
    "        submission.append({\"Query\": query, \"Assessment_url\": r.metadata[\"url\"]})\n",
    "\n",
    "submission_df = pd.DataFrame(submission)\n",
    "submission_df.to_csv(\"shl_test_predictions.csv\", index=False)\n",
    "print(\"Submission CSV created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f6cf0-fdc6-4139-8671-bad0e764a40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
